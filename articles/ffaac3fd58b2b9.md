---
title: "論文要約: XLM-RoBERTa - 100言語、2.5TBのデータで学習した多言語モデル"
emoji: "🌊"
type: "idea"
topics:
  - "論文要約"
  - "言語モデル"
published: true
published_at: "2022-05-31 20:17"
---

# はじめに

100カ国語、2.5TBのデータを使って言語モデルを学習し、多言語タスクにおいてSOTAを達成するとともに、単一言語タスクにおいても既存のモデルと同等の性能を実現することに成功した。

# 多言語モデルの課題と改善方法

本論文では、以下に示す既存の多言語モデルの課題について解析を試みた。

1) モデルの容量の制約と多言語化による転移学習効果のトレードオフ
2) 言語ごとの例文の寡多と適切なサンプリングパラメータの選択
3) モデルの容量と語彙数の関係
4) データセットのスケールと学習方法
5) Sentence Pieceによる多言語トークン化の簡略化

## 1) モデルの容量の制約と多言語化による転移学習効果のトレードオフ

モデルの容量を一定の条件では、1つのモデルを使って複数の言語の表現を学習しようとすると、1つの言語あたりに割かれる容量は小さくなる。これにより、パラメータ数固定のまま言語の数を増やすほど全体のパフォーマンスが低下する問題がある（curese of multilinguality, Fig. 2）。
一方で、多言語化によって、主にデータが少ない言語において、他の言語で学習した表現を転移してパフォーマンスが上がることが確認できた（Fig. 2）。
従って、1つのモデルが扱う言語を増やすことにはトレードオフが存在する。

## 2) 言語ごとの例文の寡多と適切なサンプリングパラメータの選択

データセットには言語ごとに例文の数が異なるため、データのサンプル比率には最適値が存在するはずである。
本論文では英語やフランス語のような例文の多い言語と、スワヒリ語やウルドゥ語のような例文の少ない言語でサンプリングのパラメータを調整し、最適なパラメータを求めた(Fig. 5)。

## 3) モデルの容量と語彙数の関係

モデルのサイズとコーパスに現れる語彙数の関係について調べた。
この結果、モデルのサイズが一定であっても語彙数が多い（32K->256K）方が2.8%の性能改善が見られた(Fig. 6)。
このことは、多言語モデルにおいてはTransformerのパラメータを削ってでもembedding layerにより多く割り当てる方が良いことを示している。

## 4) データセットのスケールと学習方法

本論文ではWikipediaのコーパスに加え、CommonCrawlのコーパスを使ってモデルを学習した。
全てのモデルにおいて、Wikipediaで学習したモデルよりも、CommonCrawlのデータを追加して学習した方が性能がよかった(Fig. 3)。

また、モデルの訓練時間を増やすことで性能が改善した。
特筆すべきは、評価データにおいてパフォーマンスが停滞した後も事前学習を続けることにより、下位タスクのパフォーマンスが向上したことである。

## 5) Sentence Pieceによる多言語トークン化の簡略化

先行研究であるmBERTやXLM-100では言語ごとのトークン化アルゴリズムを採用しているが、このことは多言語モデルが生のテキストを扱うことを難しくしている。
本論文ではSentence Piece modelを訓練し、全ての言語に対して生のテキストデータを直接モデルに入力した。
性能比較の結果では言語ごとのトークナイザーを使った場合と無視できるほどの差しかなかった(Fig. 7)。

![](https://storage.googleapis.com/zenn-user-upload/ec28a6d01782-20220531.png)
