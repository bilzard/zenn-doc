---
title: "論文解説: WaveGlow - 会話生成におけるFlowベースの生成モデル"
emoji: "🚥"
type: "idea"
topics:
  - "音声"
  - "論文解説"
  - "会話生成"
  - "生成モデル"
published: true
published_at: "2022-08-04 12:21"
---

# 論文

[WaveGlow: A Flow-based Generative Network for Speech Synthesis](https://arxiv.org/abs/1811.00002v1)

# 概要

melスペクトグラムから高品質な音声波形を生成するFlowベースの生成モデルを提案する。

# 研究分野における論文の位置付け

典型的なtest-to-speech合成は2ステップで行われる。最初のステップではテキストを時系列に配置された特徴量、すなわちMelスペクトログラムやF0周波数などの言語学的特徴量を生成する。二段階目のモデルはこれらの時系列に配置された特徴から音声サンプルを生成する。本論文では後者のモデルを扱う。

# 既存研究との違い

既存手法のGlow[^1]とWaveNet[^2]の知見を組み合わせ、高速かつ高品質な会話生成モデルを実現した。
WaveNetは高品質な会話生成が可能だが、自己回帰モデルベースの手法のため並列化が難しく、処理速度に課題がある。そこで、Glow同様のFlowベースのアーキテクチャを採用し、自己回帰不要でWaveNetと同等の品質を保ちつつ高速化を実現した。

[^1]: https://arxiv.org/abs/1807.03039 。[過去の論文解説記事](https://zenn.dev/bilzard/articles/554e37c7f3bcd2)も参照のこと。
[^2]: https://arxiv.org/abs/1609.03499

# 評価方法

提案手法のpytorch実装は、NVIDIA V100 GPU x1で507kHzのサンプルレートでの会話生成を実現した（WaveNetは0.11kHz）。また、Mean Opinion Scoreによる評価においてWaveNetと同等の品質を実現した。

**表1: 既存手法と提案手法のMOSの比較**
![](https://storage.googleapis.com/zenn-user-upload/b781183f7402-20220804.png)

# 提案手法の特徴と解説

## Melスペクトグラムを入力とする条件付き生成モデル

mel-spectrogramを条件として与えたFlowベースの生成モデルである（図1）。
基本的にはGlowと同様の構造だが、Affine coupling layerのアフィン変換パラメータの生成において、入力テンソルに加えてmelスペクトグラムの振幅を与えている（図2）。

![](https://storage.googleapis.com/zenn-user-upload/17acc9610e0d-20220804.png)
**図1: WaveGlowのアーキテクチャ**

![](https://storage.googleapis.com/zenn-user-upload/d5b78647c754-20220804.png)
**図2: Affine coupling layerの定義**

## Early outputs

全てのチャネルを全てのレイヤを通過させるのでなく、4つのcoupling layerごとに2つのチャネルを出力する。最後に各中間レイヤの出力をconcatすることで最終的な出力を得る。このような仕組みは、ネットワークの浅いレイヤに勾配を伝搬させやすくするので、Skip connectionと同等の効果が期待できる。

## 生成時の工夫

Flowベースの生成手法は生成時は潜在空間からある確率分布（通常は球面ガウス分布$\mathcal{N}(z; 0, I)$）でサンプルした$z$を入力として、モデルの逆変換を計算することで生成データ$x$を得る。

Glowや他の尤度ベースの生成手法で提案されているように、サンプルする潜在空間のガウス分布の分散を訓練時よりも小さくすることで若干精度の良い出力を得ることができる[^3]。提案手法では訓練時は$\sigma=\sqrt{0.5}$に対し、生成時は$\sigma=0.6$を用いた。

[^3]: おそらく中心から大きく外れたサンプルを得ることを防止するもの。GANにおける確率分布の両サイドをClippingするトリックと同じと思われる。

## 参考: Squeeze処理について

公式ソースコード[^4]によると、Squeeze処理では音声波形を長さ$h$のフレームでグループ化し、$w \times h$の2次元テンソルにする処理のことを指す。
このようにする意図について論文では説明されていないが、おそらく以下のような意図だと思われる。
まず、前提として、Affine couplingでは入力サンプルを2つのグループに分割し、片方のグループでアフィン変換のパラメータを推定し、もう片方のグループにアフィン変換を適用する。この時、仮にSqueezeをせずに長い音声波形をそのままAffine couplingレイヤーに入力した場合は、図3の下の図の網掛けで示したように、時間間隔が非常に離れた2つのフレームグループができる。これらは時間間隔が離れ過ぎているため、片方のグループの波形の特徴から、もう片方の波形についてのパラメータを推定するのに十分な情報を含んでいない可能性がある。一方Squeezeによってフレームを細かいチャンクに区切ることで、図3の左上に示したように、時間方向に局所的な2つのグループを作ることができる。したがって2つのグループは互いに情報の相関が強く、片方の情報からもう片方のパラメータの推定が比較的に容易にできることが期待できる。

[^6]: [WaveFlow: A Compact Flow-based Model for Raw Audio](https://arxiv.org/abs/1912.01219)

![](https://storage.googleapis.com/zenn-user-upload/715ad5d52e7e-20220805.png)
**図3: 波形のSqueeze処理**

[^4]: https://github.com/NVIDIA/waveglow
[^5]: 逆に言えば、92.8msを超える時系列データの依存性は考慮しないことを意味する。hop-sizeなどの信号処理のパラメータは、対象データのドメインに応じて適切に設定すべきである。
