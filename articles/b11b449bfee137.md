---
title: "[論文要約] Sill-Net: 照明表現の分離による特徴空間のデータ拡張"
emoji: "🙄"
type: "tech"
topics:
  - "fewshotlearning"
  - "論文要約"
published: true
published_at: "2022-03-07 17:28"
---

# 論文

[Sill-Net: Feature Augmentation with Separated Illumination Representation
](https://arxiv.org/abs/2102.03539v2)

# 概要

照明のバリエーションはオブジェクトの見た目に大きな影響を与え、ニューラルネットワークの学習を妨げる。特に、稀な照明条件のサンプルを収集することは多大な時間とコストを消費する。
本論文ではこの問題に対処するために、画像から照明の表現を分離して学習し、得られた照明の表現を既存の画像データと合成することでデータを拡張する手法を提案する。

提案したモデルは、道路標識の認識、ロゴ認識、および一般的なfew-shot classificationのタスクのいくつかにおいてSOTAを達成した。

# 提案手法の特徴

提案手法の特徴は主に以下の2点に集約される（図1）。

1. 画像から照明の表現を分離し、学習する
2. 得られた照明の表現と既存の画像（テンプレート）を合成することでデータを拡張する

![](https://storage.googleapis.com/zenn-user-upload/eaffdba278f9-20220307.png)
*図1:提案手法の概念図*

# 提案モデルのアーキテクチャ

提案モデルのアーキテクチャを図2に示す。
主に以下の5つのサブモジュールに分けられる。

1. Separation Module: 照明表現[^1]と意味表現[^2]を分離する
2. Matching Module: 抽出した意味表現がテンプレート画像に近づくように学習する
3. Reconstruction Module: 意味表現からテンプレート画像を再構成する
4. Augmentation Module: 異なるサンプルの照明表現と意味表現を合成し、新たなサンプルを生成する
5. Classifier: 分類器

なお、本手法では識別するクラスのテンプレート画像（プロトタイプ）があらかじめ与えられていることを想定している。

![](https://storage.googleapis.com/zenn-user-upload/a776fed538ce-20220307.png)
*図2:提案モデルのアーキテクチャ*

[^1]: illumination features
[^2]: semantic features

# 学習方法

1. 与えられたデータセットを元にSeparation Module, Matching Module, Reconstruction Moduleをそれぞれ学習し、分離された照明表現を照明リポジトリ[^5]に保存する。
2. サポート画像の意味表現と、照明リポジトリの照明表現をランダムに組み合わせ、データを合成する。合成されたデータセットを使って分類器を学習する。

なお、サポート画像はテンプレート画像から選択する。テンプレート画像が与えられていない場合は、訓練画像から選択する（本論文では一般的なfew-shot learningの場合にこの方法を採用）。

[^5]: illumination repository

# ロス設計

本手法では以下の3つの制約を仮定する。

1. 意味表現はテンプレート画像を再構成するのに十分な情報を持つ
2. 意味表現からラベルを予測することができるが、照明表現からはできない
3. 照明表現には意味表現が含まれてはならない

また、それぞれの制約は以下のようなロス関数として実装する。

1. Matching Loss: テンプレート画像と意味表現とのMSE誤差
2. Reconstruction Loss: 再構成後のテンプレート画像と元のテンプレート画像とのBCE誤差
3. Classification Loss: 合成後の画像を識別器に通した時のCross-Entropy誤差
4. PIDA[^3] Loss: 特定の画像の照明表現と、その画像が属するクラスの照明表現の期待値との距離を最大化する

最後のPIDAという指標は、意味表現と照明表現の分離の程度を測定するために[^4]で導入されたものである。前提として、同じクラスに属する意味表現は全サンプルで似通っているのに対し、照明表現はサンプルごとに異なることを想定している。もし仮に照明表現に意味表現の大部分が残っていた場合、照明表現の期待値とサンプルの照明表現は似通っているので、PIDAは小さくなる。従って、PIDAを最大化（PIDA誤差を最小化）することで照明表現から意味表現を除去することができる。

最終的な誤差は誤差1-4の線形結合で与える。

[^3]: Post Interventional DisAgreement
[^4]: [Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness](https://arxiv.org/abs/1811.00007)

# 実験結果

## 異なるドメインへの汎化性能

以下の異なるドメインへのfew-shot learningにおいて、既存のSOTAであるVPEを上回る精度を達成した。このことは、提案手法の汎化性能が既存の手法よりも優れていることを示す。

* 標識データセットA(GTSRB) -> 標識データセットB(TT100K)
* 標識データセット(GTSRB) -> ロゴデータセット(Flickr32, Toplogos)
* ロゴデータセットA(Belga) -> ロゴデータセットB(Flickr32, Toplogos)
 
また、一般的なfew-shot learningのタスク(miniImageNet, CUB, CIFAR-FS)においてもSOTAを達成した。

## 学習結果の可視化

学習結果を可視化したものを図3に示す。入力画像のサイズ、色、形状がテンプレート画像と異なる場合でもほぼ正確にテンプレート画像が再構成されていることがわかる。また、照明表現からラベルの特徴がほぼ含まれていないことも確認できる。

一方で、テンプレートをうまく再構成できなかった例を図4に示す。
これらの入力画像ではラベルの形状、色などが大きく歪められてしまっているため、形状変換ネットワーク[^6]がうまく対応できなかったことが原因と考えられる。従って、入力画像の歪みが大きい場合には本手法ではうまく適用できない可能性を示唆している。

![](https://storage.googleapis.com/zenn-user-upload/3ef31fd76a8b-20220307.png)
*図3: 学習結果の可視化*

![](https://storage.googleapis.com/zenn-user-upload/2b877c3fbfe0-20220307.png)
*図4: 学習結果の可視化（失敗例）*

[^6]: STN; spatial transformation network

# 所感

* 道路標識のように識別対象のプロトタイプが明確かつ数が限られる場合には本手法はうまく適用できると思われる。また、逆にプロトタイプが自明でなかったり、数が無限に存在する場合にはうまく当てはまる保証はないと思われる。
* 本論文で扱ったデータでは入力画像のサイズが64x64と極めて小さいことに注意。より大きい画像サイズを扱う場合はpoolingレイヤを挿入するなどの改変が必要になる。
* ラベルを識別するための特徴とそれ以外の情報を分離する、というのが面白い手法だと感じた。関連研究としてfeature disentanglementという分野があるらしい。
