---
title: "論文要約: Swin Transformer - 画像タスクで汎用的に使えるTransformerモデル"
emoji: "🌟"
type: "tech"
topics:
  - "画像"
  - "transformer"
  - "論文要約"
published: true
published_at: "2022-03-02 23:16"
---

# 論文

[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

# 概要

オブジェクト検出やセグメンテーションといった密な視覚タスクにおいて、従来のクラス分類に特化したモデル(ViT[^1], DeiT[^2])は解像度が十分でないことや、計算量が画像サイズの2乗に比例することが課題だった。本論文ではこれらの課題をクリアし、あらゆる画像タスクで汎用的なバックボーンとして利用できるモデルを提案する。

# 提案手法の特徴

1. 階層的な特徴抽出(FPN[^3], Unet[^4]のような構造)
3. self attentionの範囲を画像全体でなく、local window内のパッチに限定することで計算量を削減
4. sliding window でなく shifted window という手法を採用することで計算量を削減

# Shifted Window

widowを固定にすると同じwindow内のパッチ同士の相関は抽出できるが、windowをまたいだパッチの相関は抽出できない。そこで、本論文では偶数レイヤーでwindowの範囲を半分ずらす方針をとる。
こうすることでwindowの境界をまたがるパッチの相関を特徴として抽出できる。

![](https://storage.googleapis.com/zenn-user-upload/c8b1701814fc-20220302.jpeg)

# shifted window における計算量的な工夫

windowをずらしたレイヤーではwindowの数が増えてしまう。本論文では右上に位置するパッチを巡回してずらし、左下のパッチと結合することでwindowの数を一定に保っている。

![](https://storage.googleapis.com/zenn-user-upload/31103b2f5985-20220302.jpeg)

# 既存モデルとの比較

* 汎用的なモデルを意図したものだが、画像分類においても既存のTransformer (ViT/DeiT)を上回る精度を実現。また推論速度が大幅に改善。
* CNNとの比較において、imagenet-1Kで事前学習したモデルは性能はあまり変わらないが、imagenet-22Kで事前学習したモデルでは明確な差が出ている。

![](https://storage.googleapis.com/zenn-user-upload/2c410dc28815-20220302.jpeg)

---

[^1]: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
[^2]: [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2)
[^3]: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)
[^4]: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)