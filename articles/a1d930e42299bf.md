---
title: "論文解説: WaveNet - 自己回帰モデルによる高精度な音声生成モデル"
emoji: "🕸"
type: "idea"
topics:
  - "音声合成"
  - "論文解説"
  - "自己回帰モデル"
  - "会話合成"
published: true
published_at: "2022-08-04 19:54"
---

# 論文

[WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)

# 概要

コンテキストを与えて音声波形を生成する深層学習モデルを提案する。提案モデルは自己回帰的な確率モデルであり、音声サンプルの時系列を過去から未来へと順番に予測する。
提案モデルをtext-to-speechタスクに適用した場合、英語と中国語において既存の最良モデルを明らかに上回る性能を示した。
また、提案手法はコンテクストから音声波形を生成する汎用的な枠組みであり、コンテクストの与え方によって柔軟な拡張が可能である。例えば、話者のIDをコンテクストとして与えた場合は単一のモデルを使って複数の話者の会話を生成仕分けることに成功した。

# 研究の位置付け

深層生成モデルにおける自己回帰モデルベースの手法を音声合成モデルに適用可能かどうかを明らかにすることを目的とした研究である。直接的にはPixelRNN[^4]の研究成果に依拠している。PixelRNNが生成するデータは64x64=4,096と低解像度であるが、音声データは1秒間に少なくとも16,000のサンプルを生成する必要がある。提案手法はこのような高解像度な生成モデルに対してもPixelRNNのアプローチが有効であることを示した。

[^4]: [Pixel Recurrent Neural Networks](https://arxiv.org/abs/1601.06759)

# 先行研究との違い

* Text-to-speechタスクにおいて今までにない高精度な性能を示した
* 音声合成に必須な長いスパンの時間的依存性をクリアするために、非常に広い受容野を持つdilated casual convolutionに基づくアーキテクチャを開発した
* 話者のIDをコンテクストとして与えることにより、単一のモデルを使って異なる音声波形を合成することに成功した
* 同じアーキテクチャで小さな会話認識データセットに対して有効な性能を示した。また、音楽のようなドメインの異なる音声波形の生成において有望であることを示した

# 評価方法と結果

## 定量評価

北米英語および中国語（北京語）のスピーチデータセットに対して人間の被験者に対してアンケート調査を実施し、5段階評価の結果をMean Opinion Score(MOS)によって評価した。評価結果は英語、中国語ともに、既存の手法を大きく上回った(表1)。

**表1: MOSによる定量評価結果の比較**
![](https://storage.googleapis.com/zenn-user-upload/51e99c0f36d8-20220804.png)

## 定性評価

話者のIDをコンテクストに与えたケースなどが[デモページ]から確認できる。
生成された音声は極めて自然に聞こえる。

[デモページ]: https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio

# 提案手法についての解説

## Casual convolution

自己回帰な時系列確率モデルにおける「ある時点における出力値の確率分布は、それまでに出力したシーケンスの値の条件付き確率として与えられる」という仮定に基づき、時系列的に未来にあるテンソルの値をマスクして畳み込み処理を適用するというもの（図1）。

**図1: Casual convolution**
![](https://storage.googleapis.com/zenn-user-upload/44262da54bfa-20220804.png)

## Dilated convolution

ネットワークの層をあまり深くせずに受容野を広げるために、畳み込み操作においてdilation[^1]を大きくとる。具体的にはdilationを層の深さに対して2の冪乗に設定する（図2）。

[^1]: 畳み込みフィルタのパラメータを一定の間隔を開けて間引くこと、また、間引く間隔のことを意味する。dilationを大きくすることで同じパラメータ数で受容野を広げることが可能になる。参考: https://paperswithcode.com/method/dilated-convolution

**図2: Dilated convolution**
![](https://storage.googleapis.com/zenn-user-upload/d30785693173-20220804.png)

## 再量子化による情報圧縮

音声波形を16bitで量子化した場合、1フレームあたり65,536階調のバリエーションができる。これを確率モデルとして表したとき、出力として65,536次元の確率変数が必要になる。これは計算量的に大きな負担となる。これを扱いやすくするために、mu-lowアルゴリズム[^2]によって対数スケールに変換し、256階調として量子化することでデータ量を削減する。特に会話データにおいては再量子化後の波形は元の波形とほとんど変わらないと著者らは報告している。

[^2]: https://en.wikipedia.org/wiki/%CE%9C-law_algorithm

## Gated Activation

アクティベーション関数はPixelCNN[^4]に倣いGated activation(図3)を採用した。著者らによると、ReLUよりも明白に性能が良かったとのこと。

[^4]: [Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)。[要約記事](https://zenn.dev/bilzard/articles/conditional-pixelcnn)も参照。

**図3: Gated activationの定義**
![](https://storage.googleapis.com/zenn-user-upload/04f606a25bd2-20220804.png)

## アーキテクチャの概要

提案手法のアーキテクチャを図4で示す。収束を早めること、および深いネットワークの学習を安定させるために、残差接続およびSkip connectionを用いている。

**図4: WaveNetのアーキテクチャ**
![](https://storage.googleapis.com/zenn-user-upload/0f3fb523df66-20220804.png)

## コンテクストの与え方

提案手法はコンテクストに応じてデータを生成し分ける枠組みを提示する。形式的には図5で示すような条件付き確率分布で表され、実際には過去の時系列データ$x$とコンテクストのベクトル表現$h$から潜在変数$z$を予測するニューラルネットワークとして実装する。
コンテクストの与え方としてglobal contextとlocal contextの2種類を提案している。

**図5: 条件付き生成モデルの式**
![](https://storage.googleapis.com/zenn-user-upload/e2ebc3d179f1-20220804.png)

### Global context

時系列でない、固定のコンテクストを想定している（例えば、話者のIDなど）（図6）。式中の$h$はコンテクストのベクトル表現、$*$は畳み込み演算を表す。また、$V^\top h$は時間方向にブロードキャストする。

**図6: Global context**
![](https://storage.googleapis.com/zenn-user-upload/52f42cddfc21-20220804.png)

### Local context

コンテクストが時系列として与えられる場合を想定している。例として、言語学的な特徴量や音素的特徴量など（図7）。時間的な解像度は生成するデータのそれよりも粗いものを想定している。式中の$y=f(h)$はコンテクストの時系列$h$を出力の時間的解像度に合わせてアップサンプルした時系列を表す[^3]。この式では$V$は1x1 convolutionである。

[^3]: 論文ではtransposed convolution（＝パラメータを学習可能なアップサンプル）を採用した。

**図7: Local context**
![](https://storage.googleapis.com/zenn-user-upload/2613add115ce-20220804.png)

# 所感

* 「ある時点における出力は未来の入力を参照できない」という制約は必須だろうか？（bi-directionalなモデルや、Transformerのように全結合のモデルは意味をなさないのか？）
